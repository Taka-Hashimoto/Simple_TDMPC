import os
import random
import time
from dataclasses import dataclass

import flax
import flax.linen as nn
import gymnasium as gym
import jax
import jax.numpy as jnp
import numpy as np
import optax
import tyro
from flax.training.train_state import TrainState
from flax.linen.initializers import constant, orthogonal
from torch.utils.tensorboard import SummaryWriter


@dataclass
class Args:
    exp_name: str = os.path.basename(__file__)[: -len(".py")]
    """the name of this experiment"""
    seed: int = 1
    """seed of the experiment"""
    track: bool = False
    """if toggled, this experiment will be tracked with Weights and Biases"""
    wandb_project_name: str = "cleanRL"
    """the wandb's project name"""
    capture_video: bool = False
    """whether to capture videos of the agent performances (check out `videos` folder)"""
    env_id: str = 'dm_control/walker-run-v0'
    """the id of the environment"""
    action_repeat: int = 4
    """number of times the same action is repeated"""
    
    # Model learning
    total_timesteps: int = 1_000_000
    """total timesteps of the experiments"""
    learning_rate: float = 3e-4
    """the learning rate of the optimizer"""
    gamma: float = 0.95
    """the discount factor gamma"""
    tau: float = 0.01
    """target smoothing coefficient"""
    batch_size: int = 512
    """the batch size of sample from the reply memory"""
    exploration_noise: float = 0.05
    """the scale of exploration noise"""
    learning_starts: int = 25e3
    """timestep to start learning"""
    grad_clip_norm = 20.0
    """the norm for gradient clipping"""
    latent_dim: int = 512
    """coefficient of transition loss term"""
    consis_coef: float = 2.0
    """coefficient of transition loss term"""
    reward_coef: float = 0.5
    """coefficient of reward loss term"""
    value_coef: float = 0.1
    """coefficient of value loss term"""
    rho: float = 0.5
    """coefficient of loss decay over time"""
    target_network_frequency: int = 2
    "the frequency of updates for the target nerworks"

    # Planning (MPPI)
    horizon: int = 3
    """planning horizon length in time steps"""
    iterations: int = 6
    """number of updates during planning"""
    num_samples: int = 512
    """number of samples drawn from the distribution"""
    num_actor_traj: int = 16
    """number of trajectory samples generated by the actor"""
    num_elites: int = 64
    """number of elite samples for optimization"""
    temperature: float = 0.5
    """temperature parameter controlling the scale of updates"""


class Shimmy_Wrapper(gym.Wrapper):
    def __init__(self, env: gym.Env, action_repeat: int):
        super().__init__(env)
        self.action_repeat = action_repeat

    def _flatten_obs(self, obs, dtype=np.float32):
        obs_pieces = []
        for v in obs.values():
            flat = np.array([v]) if np.isscalar(v) else v.ravel()
            obs_pieces.append(flat)
        return np.concatenate(obs_pieces, axis=0).astype(dtype)

    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        return self._flatten_obs(obs), info
    
    def step(self, action):
        reward_sum = 0.0
        for _ in range(self.action_repeat):
            obs, reward, terminated, truncated, info  = self.env.step(action)
            reward_sum += (reward or 0.0)
            if terminated or truncated:
                break
        return self._flatten_obs(obs), reward_sum, terminated, truncated, info


def make_env(env_id, seed, action_repeat):
    def thunk():
        if args.capture_video:
            if env_id == 'dm_control/quadruped-run-v0':
                env = gym.make(env_id, render_mode="rgb_array", render_kwargs={"camera_id": 1})
            else:
                env = gym.make(env_id, render_mode="rgb_array", render_kwargs={"camera_id": 0})
            env = gym.wrappers.RecordVideo(
                env, 
                f"videos/{run_name}", 
                episode_trigger = lambda episode_id: episode_id % 100 == 0
            )
        else:
            env = gym.make(env_id)
        env = Shimmy_Wrapper(env, action_repeat)
        env = gym.wrappers.RecordEpisodeStatistics(env)
        env.action_space.seed(seed)
        return env

    return thunk


class ReplayBuffer():
    def __init__(self, total_timesteps, obs_dim, action_dim, horizon):
        self.horizon = horizon
        self.obs_buffer = np.empty((total_timesteps, obs_dim), dtype=np.float32)
        self.action_buffer = np.empty((total_timesteps, action_dim), dtype=np.float32)
        self.reward_buffer = np.empty((total_timesteps, 1), dtype=np.float32)
        self.termination_buffer = np.zeros((total_timesteps, ), dtype=np.bool_)
        self.iter = 0
        self.start = 0
        self.sample_flag = np.zeros((total_timesteps, ), dtype=np.bool_)

    def add(self, obs, action, reward, termination, truncation):
        self.obs_buffer[self.iter] = obs
        self.action_buffer[self.iter] = action
        self.reward_buffer[self.iter] = reward
        self.termination_buffer[self.iter] = termination
        if (self.iter - self.start) >= self.horizon:
            self.sample_flag[self.iter-self.horizon] = True
        self.iter += 1
        if termination or truncation:
            self.start = self.iter

    def sample(self, batch_size):
        idx = np.random.choice(np.where(self.sample_flag)[0], size=batch_size)
        obss = np.array([self.obs_buffer[i : i+self.horizon+1] for i in idx]).transpose(1,0,2)
        actions = np.array([self.action_buffer[i+1 : i+self.horizon+1] for i in idx]).transpose(1,0,2)
        rewards = np.array([self.reward_buffer[i+1 : i+self.horizon+1] for i in idx]).transpose(1,0,2)
        terminations = np.array([self.termination_buffer[i+1 : i+self.horizon+1] for i in idx]).transpose(1,0)
        return obss, actions, rewards, terminations


class Encoder(nn.Module):
    latent_dim: int
    @nn.compact
    def __call__(self, x):
        x = nn.Dense(256, kernel_init=orthogonal(1), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = nn.elu(x)
        x = nn.Dense(256, kernel_init=orthogonal(1), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = nn.elu(x)
        x = nn.Dense(self.latent_dim, kernel_init=orthogonal(1), bias_init=constant(0))(x)
        return x


class Dynamics(nn.Module):
    latent_dim: int
    @nn.compact
    def __call__(self, x, a):
        x = jnp.concatenate([x, a], axis=-1)
        x = nn.Dense(512, kernel_init=orthogonal(1), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = nn.elu(x)
        x = nn.Dense(512, kernel_init=orthogonal(1), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = nn.elu(x)
        x = nn.Dense(self.latent_dim, kernel_init=orthogonal(1), bias_init=constant(0))(x)
        return x 


class Reward(nn.Module):
    @nn.compact
    def __call__(self, x, a):
        x = jnp.concatenate([x, a], -1)
        x = nn.Dense(512, kernel_init=orthogonal(1), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = nn.elu(x)
        x = nn.Dense(512, kernel_init=orthogonal(1), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = nn.elu(x)
        x = nn.Dense(1, kernel_init=orthogonal(1e-5), bias_init=constant(0))(x)
        return x


class QNetwork(nn.Module):
    @nn.compact
    def __call__(self, x, a):
        x = jnp.concatenate([x, a], -1)
        x = nn.Dense(512, kernel_init=orthogonal(1), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = nn.elu(x)
        x = nn.Dense(512, kernel_init=orthogonal(1), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = nn.elu(x)
        x = nn.Dense(1, kernel_init=orthogonal(1e-5), bias_init=constant(0))(x)
        return x


class Actor(nn.Module):
    action_dim: int

    @nn.compact
    def __call__(self, x):
        x = nn.Dense(512, kernel_init=orthogonal(1), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = nn.elu(x)
        x = nn.Dense(512, kernel_init=orthogonal(1), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = nn.elu(x)
        x = nn.Dense(self.action_dim, kernel_init=orthogonal(1), bias_init=constant(0))(x)
        x = nn.tanh(x)
        return x


class TrainState(TrainState):
    target_params: flax.core.FrozenDict


if __name__ == "__main__":
    args = tyro.cli(Args)
    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
    group_name = f"{args.env_id}__{args.exp_name}"

    if args.track:
        import wandb

        wandb.init(
            project=args.wandb_project_name,
            group=group_name,
            sync_tensorboard=True,
            config=vars(args),
            name=run_name,
            monitor_gym=True,
            save_code=True,
        )
    writer = SummaryWriter(f"runs/{run_name}")
    writer.add_text(
        "hyperparameters",
        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
    )

    random.seed(args.seed)
    np.random.seed(args.seed)
    key = jax.random.PRNGKey(args.seed)
    key, encoder_key, dynamics_key, reward_key, actor_key, qf1_key, qf2_key = jax.random.split(key, 7)

    # env setup
    env = make_env(args.env_id, args.seed, args.action_repeat)()
    assert isinstance(env.action_space, gym.spaces.Box), "only continuous action space is supported"

    obs, info = env.reset()
    obs_dim = len(obs)
    action_dim = env.action_space.shape[0]

    encoder = Encoder(args.latent_dim)
    encoder_state = TrainState.create(
        apply_fn = encoder.apply,
        params = encoder.init(encoder_key, jnp.zeros(obs_dim)),
        target_params = encoder.init(encoder_key, jnp.zeros(obs_dim)),
        tx = optax.chain(
            optax.clip_by_global_norm(args.grad_clip_norm),
            optax.adam(learning_rate=args.learning_rate)
        )
    )
    dynamics = Dynamics(args.latent_dim)
    dynamics_state = TrainState.create(
        apply_fn = dynamics.apply,
        params = dynamics.init(dynamics_key, jnp.zeros(args.latent_dim), jnp.zeros(action_dim)),
        target_params = None,
        tx = optax.chain(
            optax.clip_by_global_norm(args.grad_clip_norm),
            optax.adam(learning_rate=args.learning_rate)
        )
    )
    reward_fn = Reward()
    reward_state = TrainState.create(
        apply_fn = reward_fn.apply,
        params = reward_fn.init(reward_key, jnp.zeros(args.latent_dim), jnp.zeros(action_dim)),
        target_params = None,
        tx = optax.chain(
            optax.clip_by_global_norm(args.grad_clip_norm),
            optax.adam(learning_rate=args.learning_rate)
        )
    )
    actor = Actor(action_dim)
    actor_state = TrainState.create(
        apply_fn = actor.apply,
        params = actor.init(actor_key, jnp.ones(args.latent_dim)),
        target_params = actor.init(actor_key, jnp.ones(args.latent_dim)),
        tx = optax.chain(
            optax.clip_by_global_norm(args.grad_clip_norm),
            optax.adam(learning_rate=args.learning_rate)
        )
    )
    qf = QNetwork()
    qf1_state = TrainState.create(
        apply_fn = qf.apply,
        params = qf.init(qf1_key, jnp.zeros(args.latent_dim), jnp.zeros(action_dim)),
        target_params = qf.init(qf1_key, jnp.zeros(args.latent_dim), jnp.zeros(action_dim)),
        tx = optax.chain(
            optax.clip_by_global_norm(args.grad_clip_norm),
            optax.adam(learning_rate=args.learning_rate)
        )
    )

    qf2_state = TrainState.create(
        apply_fn = qf.apply,
        params = qf.init(qf2_key, jnp.zeros(args.latent_dim), jnp.zeros(action_dim)),
        target_params = qf.init(qf2_key, jnp.zeros(args.latent_dim), jnp.zeros(action_dim)),
        tx = optax.chain(
            optax.clip_by_global_norm(args.grad_clip_norm),
            optax.adam(learning_rate=args.learning_rate)
        )
    )
    encoder.apply = jax.jit(encoder.apply)
    dynamics.apply = jax.jit(dynamics.apply)
    reward_fn.apply = jax.jit(reward_fn.apply)
    actor.apply = jax.jit(actor.apply)
    qf.apply = jax.jit(qf.apply)

    @jax.jit
    def update_dynamics_and_critic(
        encoder_state: TrainState,
        dynamics_state: TrainState,
        reward_state: TrainState,
        actor_state: TrainState,
        qf1_state: TrainState,
        qf2_state: TrainState,
        obss: np.ndarray,
        actions: np.ndarray,
        rewards: np.ndarray,
        terminations: np.ndarray,
    ):
        def calc_target_value(next_latent, reward, termination):
            next_action = actor.apply(actor_state.target_params, next_latent)
            qf1_next_target = qf.apply(qf1_state.target_params, next_latent, next_action)
            qf2_next_target = qf.apply(qf2_state.target_params, next_latent, next_action)
            min_qf_next_target = jnp.minimum(qf1_next_target, qf2_next_target)
            qf_target = reward + (1 - termination[:, None]) * args.gamma * min_qf_next_target
            return qf_target

        f_vmap = jax.vmap(encoder.apply, (None, 0))
        next_latents = f_vmap(encoder_state.target_params, obss[1:])
        qf_targets = jax.vmap(calc_target_value)(next_latents, rewards, terminations)

        def loss_fn(encoder_params, dynamics_params, reward_params, qf1_params, qf2_params):

            def rollout(latent, action):
                latent_pred = dynamics.apply(dynamics_params, latent, action)
                reward_pred = reward_fn.apply(reward_params, latent, action) # fix
                return latent_pred, (latent_pred, reward_pred)

            latent_init = encoder.apply(encoder_params, obss[0])
            _, (latents_pred, rewards_pred) = jax.lax.scan(rollout, latent_init, actions)
            
            rhos = args.rho ** jnp.arange(args.horizon)
            rhos = jnp.expand_dims(rhos, axis=(1,2))

            transition_loss = jnp.mean(rhos * (next_latents - latents_pred) ** 2)
            reward_loss = jnp.mean(rhos * (rewards - rewards_pred) ** 2)
            
            latents_pred_ = jnp.concatenate([latent_init[None], latents_pred[:-1]])
            qf_vmap = jax.vmap(qf.apply, in_axes=(None, 0, 0))
            qf1_values = qf_vmap(qf1_params, latents_pred_, actions)
            qf2_values = qf_vmap(qf2_params, latents_pred_, actions)
            
            qf1_loss = jnp.mean(rhos * (qf1_values - qf_targets)**2)
            qf2_loss = jnp.mean(rhos * (qf2_values - qf_targets)**2)

            total_loss = args.consis_coef * transition_loss + args.reward_coef * reward_loss + args.value_coef * (qf1_loss + qf2_loss)
            return total_loss, (latents_pred, transition_loss, reward_loss, qf1_loss, qf2_loss, qf1_values, qf2_values)

        grad_fn = jax.value_and_grad(loss_fn, argnums=(0,1,2,3,4), has_aux=True)
        (_, (latents_pred, transition_loss, reward_loss, qf1_loss, qf2_loss, qf1_values, qf2_values)), grads = \
            grad_fn(encoder_state.params, dynamics_state.params, reward_state.params, qf1_state.params, qf2_state.params)

        encoder_state = encoder_state.apply_gradients(grads=grads[0])
        dynamics_state = dynamics_state.apply_gradients(grads=grads[1])
        reward_state = reward_state.apply_gradients(grads=grads[2])
        qf1_state = qf1_state.apply_gradients(grads=grads[3])
        qf2_state = qf2_state.apply_gradients(grads=grads[4])
        
        return (encoder_state, dynamics_state, reward_state, qf1_state, qf2_state), (transition_loss, reward_loss, qf1_loss, qf2_loss), (qf1_values, qf2_values), latents_pred


    @jax.jit
    def update_actor(
        actor_state: TrainState,
        qf1_state: TrainState,
        qf2_state: TrainState,
        latents_pred: jnp.ndarray,
    ):
        def loss_fn(params):
            def one_step_loss(latent):
                action = actor.apply(params, latent)
                qf1_value = qf.apply(qf1_state.params, latent, action)
                qf2_value = qf.apply(qf2_state.params, latent, action)
                min_qf_value = jnp.minimum(qf1_value, qf2_value)
                return - jnp.mean(min_qf_value)

            rhos = args.rho ** jnp.arange(args.horizon)
            actor_loss_sequence = jax.vmap(one_step_loss)(latents_pred)
            return jnp.mean(rhos * actor_loss_sequence)

        loss_value, grads = jax.value_and_grad(loss_fn)(actor_state.params)
        actor_state = actor_state.apply_gradients(grads=grads)
        return actor_state, loss_value
    

    @jax.jit
    def update_target(encoder_state, qf1_state, qf2_state, actor_state):
        encoder_state = encoder_state.replace(target_params=optax.incremental_update(encoder_state.params, encoder_state.target_params, args.tau))
        qf1_state = qf1_state.replace(target_params=optax.incremental_update(qf1_state.params, qf1_state.target_params, args.tau))
        qf2_state = qf2_state.replace(target_params=optax.incremental_update(qf2_state.params, qf2_state.target_params, args.tau))
        actor_state = actor_state.replace(target_params=optax.incremental_update(actor_state.params, actor_state.target_params, args.tau))
        return encoder_state, qf1_state, qf2_state, actor_state


    @jax.jit
    def planning_fn(
        key: jnp.ndarray,
        obs: np.ndarray,
        encoder_state: TrainState,
        dynamics_state: TrainState,
        reward_state: TrainState,
        actor_state: TrainState,
        qf1_state: TrainState,
        qf2_state: TrainState,
        actions_mean: jnp.ndarray,
    ):

        latent_init = encoder.apply(encoder_state.params, obs)

        def get_actor_action_sequence(key):
            def scan_fn(latent, key):
                action = actor.apply(actor_state.params, latent)
                eps = jax.random.normal(key, (args.num_actor_traj, action_dim))
                action = action + args.exploration_noise * eps
                action = jnp.clip(action, -1.0, 1.0)
                latent_pred = dynamics.apply(dynamics_state.params, latent, action)
                return latent_pred, action

            keys = jax.random.split(key, args.horizon)
            latent_init_batch = jnp.repeat(latent_init[None], args.num_actor_traj, axis=0)
            _, actions_batch = jax.lax.scan(scan_fn, latent_init_batch, keys)
            actions_batch = jnp.transpose(actions_batch, (1, 0, 2))
            return actions_batch

        key, subkey = jax.random.split(key)
        action_actor_batch = get_actor_action_sequence(subkey)

        def sample_action_sequence(key, actions_mean, actions_std):
            eps = jax.random.normal(key, (args.num_samples, args.horizon, action_dim))
            actions_batch = actions_mean[None] + actions_std[None] * eps
            actions_batch = jnp.clip(actions_batch, -1.0, 1.0)
            return actions_batch

        def compute_return(latent, actions):
            def rollout(carry, action):
                latent, discount = carry
                next_latent = dynamics.apply(dynamics_state.params, latent, action)
                reward = reward_fn.apply(reward_state.params, latent, action)
                return (next_latent, args.gamma*discount), discount*reward

            (latent_last, discount), rewards = jax.lax.scan(rollout, (latent, 1.0), actions)
            action = actor.apply(actor_state.params, latent_last)
            qf1_value = qf.apply(qf1_state.params, latent_last, action).squeeze()
            qf2_value = qf.apply(qf2_state.params, latent_last, action).squeeze()
            V = jnp.sum(rewards) + discount * jnp.minimum(qf1_value, qf2_value)
            return V

        def get_elite(key, actions_mean, actions_std):
            key, subkey = jax.random.split(key)
            actions_batch = jnp.concatenate([
                sample_action_sequence(subkey, actions_mean, actions_std),
                action_actor_batch,
            ])
            V_func = lambda actions: compute_return(latent_init, actions)
            V_batch = jax.vmap(V_func)(actions_batch)
            V_elite, elite_index = jax.lax.top_k(V_batch, args.num_elites)
            actions_elite = actions_batch[elite_index]
            return V_elite, actions_elite

        def update_params(mean, std, V_elite, actions_elite):
            score = jnp.exp(args.temperature * (V_elite - jnp.max(V_elite)))
            score = score[:, None, None] / (jnp.sum(score) + 1e-9)
            mean = jnp.sum(score * actions_elite, axis=0)
            std = jnp.sqrt(jnp.sum(score * (actions_elite - mean[None])**2, axis=0))
            return mean, std

        def body_fun(_, val):
            key, actions_mean, actions_std = val
            key, subkey = jax.random.split(key)
            V_elite, actions_elite = get_elite(subkey, actions_mean, actions_std)
            actions_mean, actions_std \
                = update_params(actions_mean, actions_std, V_elite, actions_elite)
            return (key, actions_mean, actions_std)

        actions_std = jnp.ones((args.horizon, action_dim))
        init_val = (key, actions_mean, actions_std)
        val = jax.lax.fori_loop(0, args.iterations, body_fun, init_val)
        key, actions_mean, actions_std = val

        key, subkey = jax.random.split(key)
        V_elite, actions_elite = get_elite(subkey, actions_mean, actions_std)
        action_best = actions_elite[jnp.argmax(V_elite), 0]

        return action_best, actions_mean
    
    rb = ReplayBuffer(
        args.total_timesteps, 
        obs_dim, 
        action_dim,
        args.horizon
    )

    start_time = time.time()
    actions_mean = jnp.zeros((args.horizon, action_dim))
    termination, truncation = True, True
    total_update = 0

    for global_step in range(0, args.total_timesteps, args.action_repeat):

        if not (termination or truncation):

            if global_step < args.learning_starts:
                action = env.action_space.sample()

            else:
                key, subkey1, subkey2 = jax.random.split(key, 3)
                action, actions_mean = planning_fn(
                    subkey1,
                    obs,
                    encoder_state,
                    dynamics_state,
                    reward_state,
                    actor_state,
                    qf1_state,
                    qf2_state,
                    actions_mean,
                )
                actions_mean = jnp.concatenate([actions_mean[1:], jnp.zeros((1, action_dim))], axis=0)
                eps = jax.random.normal(subkey2, (action_dim, ))
                action = (action + args.exploration_noise * eps).clip(-1,1)
                action = np.array(action)

            obs, reward, termination, truncation, info = env.step(action)
            rb.add(obs, action, reward, termination, truncation)

        else:
            if 'episode' in info:
                print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
                
            obs, _ = env.reset()
            actions_mean = jnp.zeros((args.horizon, action_dim))
            termination, truncation = False, False
            rb.add(obs, np.zeros(action_dim), np.zeros(1), termination, truncation)


        if global_step > args.learning_starts:

            obss, actions, rewards, terminations = rb.sample(args.batch_size)
            (encoder_state, dynamics_state, reward_state, qf1_state, qf2_state), (transition_loss, reward_loss, qf1_loss, qf2_loss), (qf1_values, qf2_values), latents_pred = \
                update_dynamics_and_critic(
                    encoder_state, 
                    dynamics_state, 
                    reward_state, 
                    actor_state, 
                    qf1_state, 
                    qf2_state, 
                    obss, 
                    actions, 
                    rewards,
                    terminations,
                )

            actor_state, actor_loss = \
                update_actor(actor_state, qf1_state, qf2_state, latents_pred)
        
            if total_update % args.target_network_frequency == 0:
                encoder_state, qf1_state, qf2_state, actor_state = \
                        update_target(encoder_state, qf1_state, qf2_state, actor_state)

            if total_update % 100 == 0:
                writer.add_scalar("losses/transition_loss", transition_loss.item(), global_step)
                writer.add_scalar("losses/reward_loss", reward_loss.item(), global_step)
                writer.add_scalar("charts/qfs_values", np.mean(qf1_values + qf2_values).item() / 2.0, global_step)
                writer.add_scalar("losses/qfs_loss", (qf1_loss + qf2_loss).item() / 2.0, global_step)
                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
                print("SPS:", int(global_step / (time.time() - start_time)))
                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)

            total_update += 1

    env.close()
    writer.close()
