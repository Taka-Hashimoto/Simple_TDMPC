import os
import random
import time
from dataclasses import dataclass

import flax
import flax.linen as nn
import gymnasium as gym
import jax
import jax.numpy as jnp
import numpy as np
import optax
import tyro
from flax.training.train_state import TrainState
from flax.linen.initializers import constant, truncated_normal
from torch.utils.tensorboard import SummaryWriter


@dataclass
class Args:
    exp_name: str = os.path.basename(__file__)[: -len(".py")]
    """the name of this experiment"""
    seed: int = 1
    """seed of the experiment"""
    track: bool = False
    """if toggled, this experiment will be tracked with Weights and Biases"""
    wandb_project_name: str = "Simple_TDMPC"
    """the wandb's project name"""
    capture_video: bool = False
    """whether to capture videos of the agent performances (check out `videos` folder)"""
    env_id: str = 'dm_control/cartpole-swingup-v0'
    """the id of the environment"""
    action_repeat: int = 4
    """number of times the same action is repeated"""
    
    # Model learning
    total_timesteps: int = 100_000
    """total timesteps of the experiments"""
    encoder_learning_rate: float = 1e-4
    """the learning rate of the encoder"""
    learning_rate: float = 3e-4
    """the learning rate of the optimizer"""
    gamma: float = 0.95
    """the discount factor gamma"""
    tau: float = 0.01
    """target smoothing coefficient"""
    batch_size: int = 512
    """the batch size of sample from the reply memory"""
    exploration_noise: float = 0.05
    """the scale of exploration noise"""
    learning_starts: int = 25e3
    """timestep to start learning"""
    grad_clip_norm = 20.0
    """the norm for gradient clipping"""
    latent_dim: int = 512
    """coefficient of transition loss term"""
    consis_coef: float = 20.0
    """coefficient of transition loss term"""
    reward_coef: float = 0.1
    """coefficient of reward loss term"""
    value_coef: float = 0.1
    """coefficient of value loss term"""
    entropy_coef: float = 1e-4
    """coefficient of entropy loss term"""
    rho: float = 0.5
    """coefficient of loss decay over time"""
    target_network_frequency: int = 2
    """the frequency of updates for the target nerworks"""
    simnorm_dim: int = 8
    """the number of simnorm dimension"""
    critic_num: int = 5
    """the number of critic function"""
    dropout: float = 0.01
    """the degree of dropout"""
    vmin: float = -10.0
    """minimum value of pre-activated reward"""
    vmax: float =  10.0
    """maximum value of pre-activated reward"""
    num_bins: int = 101
    """num of bins for reward discritization"""

    # Planning (MPPI)
    horizon: int = 3
    """planning horizon length in time steps"""
    iterations: int = 6
    """number of updates during planning"""
    num_samples: int = 512
    """number of samples drawn from the distribution"""
    num_actor_traj: int = 16
    """number of trajectory samples generated by the actor"""
    num_elites: int = 64
    """number of elite samples for optimization"""
    temperature: float = 0.5
    """temperature parameter controlling the scale of updates"""
    min_std: float = 0.05
    """minimum std for exploration noise"""
    max_std: float = 2.0
    """maximum std for exploration noise"""


class Shimmy_Wrapper(gym.Wrapper):
    def __init__(self, env: gym.Env, action_repeat: int):
        super().__init__(env)
        self.metadata["render_fps"] = int(1.0/self.get_wrapper_attr('dt'))
        self.action_repeat = action_repeat

    def _flatten_obs(self, obs, dtype=np.float32):
        obs_pieces = []
        for v in obs.values():
            flat = np.array([v]) if np.isscalar(v) else v.ravel()
            obs_pieces.append(flat)
        return np.concatenate(obs_pieces, axis=0).astype(dtype)

    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        return self._flatten_obs(obs), info
    
    def step(self, action):
        reward_sum = 0.0
        for _ in range(self.action_repeat):
            obs, reward, terminated, truncated, info  = self.env.step(action)
            reward_sum += (reward or 0.0)
            if terminated or truncated:
                break
        return self._flatten_obs(obs), reward_sum, terminated, truncated, info


def make_env(env_id, seed, action_repeat):
    def thunk():
        if args.capture_video:
            if env_id == 'dm_control/quadruped-run-v0':
                env = gym.make(env_id, render_mode="rgb_array", render_kwargs={"camera_id": 1})
            else:
                env = gym.make(env_id, render_mode="rgb_array", render_kwargs={"camera_id": 0})
            env = gym.wrappers.RecordVideo(
                env, 
                f"videos/{run_name}", 
                episode_trigger = lambda episode_id: episode_id % 20 == 0
            )
        else:
            env = gym.make(env_id)

        env = Shimmy_Wrapper(env, action_repeat)
        env = gym.wrappers.RecordEpisodeStatistics(env)
        env.action_space.seed(seed)
        return env
    return thunk


class ReplayBuffer():
    def __init__(self, total_timesteps, obs_dim, action_dim, horizon):
        self.horizon = horizon
        self.obs_buffer = np.empty((total_timesteps, obs_dim), dtype=np.float32)
        self.action_buffer = np.empty((total_timesteps, action_dim), dtype=np.float32)
        self.reward_buffer = np.empty((total_timesteps, 1), dtype=np.float32)
        self.termination_buffer = np.zeros((total_timesteps, ), dtype=np.bool_)
        self.iter = 0
        self.start = 0
        self.sample_flag = np.zeros((total_timesteps, ), dtype=np.bool_)

    def add(self, obs, action, reward, termination, truncation):
        self.obs_buffer[self.iter] = obs
        self.action_buffer[self.iter] = action
        self.reward_buffer[self.iter] = reward
        self.termination_buffer[self.iter] = termination
        if (self.iter - self.start) >= self.horizon:
            self.sample_flag[self.iter-self.horizon] = True
        self.iter += 1
        if termination or truncation:
            self.start = self.iter

    def sample(self, batch_size):
        idx = np.random.choice(np.where(self.sample_flag)[0], size=batch_size)
        obss = np.array([self.obs_buffer[i : i+self.horizon+1] for i in idx]).transpose(1,0,2)
        actions = np.array([self.action_buffer[i+1 : i+self.horizon+1] for i in idx]).transpose(1,0,2)
        rewards = np.array([self.reward_buffer[i+1 : i+self.horizon+1] for i in idx]).transpose(1,0,2)
        terminations = np.array([self.termination_buffer[i+1 : i+self.horizon+1] for i in idx]).transpose(1,0)
        return obss, actions, rewards, terminations
    

class SimNorm(nn.Module):
    simnorm_dim: int
    @nn.compact
    def __call__(self, x):
        shp = x.shape
        x = x.reshape(*shp[:-1], -1, self.simnorm_dim)
        x = jax.nn.softmax(x, axis=-1)
        return x.reshape(*shp)


class Mish(nn.Module):
    @nn.compact
    def __call__(self, x):
        return x * jax.nn.tanh(jax.nn.softplus(x))


class Encoder(nn.Module):
    latent_dim: int
    simnorm_dim: int
    @nn.compact
    def __call__(self, x):
        x = nn.Dense(256, kernel_init=truncated_normal(stddev=0.02), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = Mish()(x)
        x = nn.Dense(256, kernel_init=truncated_normal(stddev=0.02), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = Mish()(x)
        x = nn.Dense(self.latent_dim, kernel_init=truncated_normal(stddev=0.02), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = SimNorm(self.simnorm_dim)(x)
        return x


class Dynamics(nn.Module):
    latent_dim: int
    simnorm_dim: int
    @nn.compact
    def __call__(self, x, a):
        x = jnp.concatenate([x, a], axis=-1)
        x = nn.Dense(512, kernel_init=truncated_normal(stddev=0.02), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = Mish()(x)
        x = nn.Dense(512, kernel_init=truncated_normal(stddev=0.02), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = Mish()(x)
        x = nn.Dense(self.latent_dim, kernel_init=truncated_normal(stddev=0.02), bias_init=constant(0))(x)
        x = SimNorm(self.simnorm_dim)(x)
        return x


class Reward(nn.Module):
    num_bins: int
    @nn.compact
    def __call__(self, x, a):
        x = jnp.concatenate([x, a], -1)
        x = nn.Dense(512, kernel_init=truncated_normal(stddev=0.02), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = Mish()(x)
        x = nn.Dense(512, kernel_init=truncated_normal(stddev=0.02), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = Mish()(x)
        x = nn.Dense(self.num_bins, kernel_init=constant(0), bias_init=constant(0))(x)
        return x


class QNetwork(nn.Module):
    num_bins: int
    dropout: float
    @nn.compact
    def __call__(self, x, a, training):
        x = jnp.concatenate([x, a], -1)
        x = nn.Dense(512, kernel_init=truncated_normal(stddev=0.02), bias_init=constant(0))(x)
        x = nn.Dropout(rate=self.dropout, deterministic=not training)(x)
        x = nn.LayerNorm()(x)
        x = Mish()(x)
        x = nn.Dense(512, kernel_init=truncated_normal(stddev=0.02), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = Mish()(x)
        x = nn.Dense(self.num_bins, kernel_init=constant(0), bias_init=constant(0))(x)
        return x


class Actor(nn.Module):
    action_dim: int

    @nn.compact
    def __call__(self, x):
        x = nn.Dense(512, kernel_init=truncated_normal(stddev=0.02), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = Mish()(x)
        x = nn.Dense(512, kernel_init=truncated_normal(stddev=0.02), bias_init=constant(0))(x)
        x = nn.LayerNorm()(x)
        x = Mish()(x)
        mu = nn.Dense(self.action_dim, kernel_init=truncated_normal(stddev=0.02), bias_init=constant(0))(x)
        log_std = nn.Dense(self.action_dim, kernel_init=truncated_normal(stddev=0.02), bias_init=constant(0))(x)
        return mu, log_std


class TrainState(TrainState):
    target_params: flax.core.FrozenDict


if __name__ == "__main__":
    args = tyro.cli(Args)
    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
    group_name = f"{args.env_id}__{args.exp_name}"

    if args.track:
        import wandb

        wandb.init(
            project=args.wandb_project_name,
            group=group_name,
            sync_tensorboard=True,
            config=vars(args),
            name=run_name,
            monitor_gym=True,
            save_code=True,
        )
    writer = SummaryWriter(f"runs/{run_name}")
    writer.add_text(
        "hyperparameters",
        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
    )

    random.seed(args.seed)
    np.random.seed(args.seed)
    key = jax.random.PRNGKey(args.seed)
    key, encoder_key, dynamics_key, reward_key, actor_key, qf_key = jax.random.split(key, 6)

    env = make_env(args.env_id, args.seed, args.action_repeat)()
    assert isinstance(env.action_space, gym.spaces.Box), "only continuous action space is supported"

    obs, info = env.reset()
    obs_dim = len(obs)
    action_dim = env.action_space.shape[0]

    encoder = Encoder(args.latent_dim, args.simnorm_dim)
    encoder_state = TrainState.create(
        apply_fn = encoder.apply,
        params = encoder.init(encoder_key, jnp.zeros(obs_dim)),
        target_params = encoder.init(encoder_key, jnp.zeros(obs_dim)),
        tx = optax.chain(
            optax.clip_by_global_norm(args.grad_clip_norm),
            optax.adam(learning_rate=args.encoder_learning_rate)
        )
    )
    dynamics = Dynamics(args.latent_dim, args.simnorm_dim)
    dynamics_state = TrainState.create(
        apply_fn = dynamics.apply,
        params = dynamics.init(dynamics_key, jnp.zeros(args.latent_dim), jnp.zeros(action_dim)),
        target_params = None,
        tx = optax.chain(
            optax.clip_by_global_norm(args.grad_clip_norm),
            optax.adam(learning_rate=args.learning_rate)
        )
    )
    reward_fn = Reward(args.num_bins)
    reward_state = TrainState.create(
        apply_fn = reward_fn.apply,
        params = reward_fn.init(reward_key, jnp.zeros(args.latent_dim), jnp.zeros(action_dim)),
        target_params = None,
        tx = optax.chain(
            optax.clip_by_global_norm(args.grad_clip_norm),
            optax.adam(learning_rate=args.learning_rate)
        )
    )
    actor = Actor(action_dim)
    actor_state = TrainState.create(
        apply_fn = actor.apply,
        params = actor.init(actor_key, jnp.ones(args.latent_dim)),
        target_params = actor.init(actor_key, jnp.ones(args.latent_dim)),
        tx = optax.chain(
            optax.clip_by_global_norm(args.grad_clip_norm),
            optax.adam(learning_rate=args.learning_rate)
        )
    )
    qf = QNetwork(args.num_bins, args.dropout)
    qfs_key = jax.random.split(qf_key, args.critic_num)
    qfs_state = TrainState.create(
        apply_fn = qf.apply,
        params = jax.vmap(qf.init, (0, None, None, None))(qfs_key, jnp.zeros(args.latent_dim), jnp.zeros(action_dim), False),
        target_params = jax.vmap(qf.init, (0, None, None, None))(qfs_key, jnp.zeros(args.latent_dim), jnp.zeros(action_dim), False),
        tx = optax.chain(
            optax.clip_by_global_norm(args.grad_clip_norm),
            optax.adam(learning_rate=args.learning_rate)
        )
    )
    encoder.apply = jax.jit(encoder.apply)
    dynamics.apply = jax.jit(dynamics.apply)
    reward_fn.apply = jax.jit(reward_fn.apply)
    actor.apply = jax.jit(actor.apply)

    def symlog(x):
        x = jnp.sign(x) * jnp.log(1 + jnp.abs(x))
        return x

    def symexp(x):
        x = jnp.sign(x) * (jnp.exp(jnp.abs(x)) - 1)
        return x

    def two_hot(x):
        bin_size = (args.vmax - args.vmin) / (args.num_bins - 1)
        x = symlog(x)
        x = jnp.clip(x, args.vmin, args.vmax)
        bin_idx = jnp.floor((x - args.vmin) / bin_size).astype(jnp.int32)
        bin_offsets = ((x - args.vmin) / bin_size - bin_idx)

        def func(bin_id, bin_offset):
            temp = jnp.zeros((args.num_bins,))
            temp = temp.at[bin_id].set(1 - bin_offset)
            temp = temp.at[(bin_id+1) % args.num_bins].set(bin_offset)
            return temp
        
        soft_two_hot = jax.vmap(func)(bin_idx, bin_offsets)
        return soft_two_hot

    def two_hot_inv(x):
        x = jax.nn.softmax(x, axis=-1)
        bins = jnp.linspace(args.vmin, args.vmax, args.num_bins)
        x = jnp.sum(x * bins, axis=-1, keepdims=True)
        return symexp(x)

    def soft_ce(logits, targets):
        pred = jax.nn.log_softmax(logits, axis=-1)
        targets_disc = jax.vmap(two_hot)(targets)
        return - jnp.sum(targets_disc * pred, axis=-1) 

    positions = np.array([0.05, 0.95]) * (args.batch_size - 1)
    floored = np.floor(positions)
    ceiled = floored + 1
    ceiled[ceiled > args.batch_size - 1] = args.batch_size - 1
    weight_ceiled = positions - floored
    weight_floored = 1.0 - weight_ceiled

    def update_scale(batch_q, scale):
        in_sorted = jnp.sort(batch_q.squeeze(), axis=0)
        d0 = in_sorted[floored.astype(np.uint32)] * weight_floored
        d1 = in_sorted[ceiled.astype(np.uint32)] * weight_ceiled
        percentiles = d0 + d1
        value = jnp.clip(percentiles[1] - percentiles[0], 1.0)
        return  scale + args.tau * (value - scale)
    
    def gaussian_logprob(eps, log_std):
        residual = jnp.sum(-0.5 * eps**2 - log_std, axis=-1, keepdims=True)
        return (residual - 0.5 * jnp.log(2 * jnp.pi)) * eps.shape[-1]


    def squash(mu, pi, log_pi):
        mu = jax.nn.tanh(mu)
        pi = jax.nn.tanh(pi)
        log_pi -= jnp.sum(jnp.log(jax.nn.relu(1 - pi**2) + 1e-6), axis=-1, keepdims=True)
        return mu, pi, log_pi


    def actor_forward(actor_params, key, z, log_std_min=-10.0, log_std_max=2.0):
        mu, log_std = actor.apply(actor_params, z)
        log_std = log_std_min + 0.5 * (log_std_max - log_std_min) * (jnp.tanh(log_std) + 1)
        eps = jax.random.normal(key, mu.shape)
        pi = mu + eps * jnp.exp(log_std)
        log_pi = gaussian_logprob(eps, log_std)
        mu, pi, log_pi = squash(mu, pi, log_pi)
        return mu, pi, log_pi, log_std

    @jax.jit
    def update_dynamics_and_critic(
        key: jnp.ndarray,
        encoder_state: TrainState,
        dynamics_state: TrainState,
        reward_state: TrainState,
        actor_state: TrainState,
        qfs_state: TrainState,
        obss: np.ndarray,
        actions: np.ndarray,
        rewards: np.ndarray,
        terminations: np.ndarray,
    ):
        
        def calc_target_value(key, next_latent, reward, termination):
            key1, key2, key3 = jax.random.split(key, 3)
            next_action = actor_forward(actor_state.params, key1, next_latent)[1]
            dropout_keys = jax.random.split(key2, args.critic_num)
            f = lambda param, key: qf.apply(param, next_latent, next_action, True, rngs={'dropout': key})
            qfs_disc_value = jax.vmap(f)(qfs_state.target_params, dropout_keys)
            qf1_disc_value, qf2_disc_value = qfs_disc_value[jax.random.choice(key3, args.critic_num, (2,), replace=False)]
            qf1_value, qf2_value = two_hot_inv(qf1_disc_value), two_hot_inv(qf2_disc_value)
            min_qf_next_target = jnp.minimum(qf1_value, qf2_value)
            qf_target = reward + (1 - termination[:, None]) * args.gamma * min_qf_next_target
            return qf_target

        next_latents = encoder.apply(encoder_state.params, obss[1:])

        key, subkey = jax.random.split(key)
        keys = jax.random.split(subkey, len(next_latents))
        qf_targets = jax.vmap(calc_target_value)(keys, next_latents, rewards, terminations)

        def loss_fn(encoder_params, dynamics_params, reward_params, qfs_params):

            def rollout(latent, action):
                latent_pred = dynamics.apply(dynamics_params, latent, action)
                reward_pred = reward_fn.apply(reward_params, latent, action)
                return latent_pred, (latent_pred, reward_pred)

            latent_init = encoder.apply(encoder_params, obss[0])
            _, (latents_pred, rewards_pred) = jax.lax.scan(rollout, latent_init, actions)
            
            rhos = args.rho ** jnp.arange(args.horizon)
            transition_loss = jnp.mean(rhos[:, None, None] * (next_latents - latents_pred) ** 2)    
            reward_loss = jnp.mean(rhos[:, None] * soft_ce(rewards_pred, rewards))
            
            latents_pred = jnp.concatenate([latent_init[None], latents_pred])
            dropout_keys = jax.random.split(key, args.critic_num)

            vmap_qf = lambda param, latent, action, key: qf.apply(param, latent, action, True, rngs={'dropout': key})
            f = lambda param, latents, actions, key: jax.vmap(vmap_qf, (None, 0, 0, None))(param, latents, actions, key)
            qfs_values = jax.vmap(f, (0, None, None, 0))(qfs_params, latents_pred[:-1], actions, dropout_keys)

            value_loss = jnp.mean(rhos[None, :, None] * jax.vmap(soft_ce, (0, None))(qfs_values, qf_targets))

            total_loss = args.consis_coef * transition_loss + args.reward_coef * reward_loss + args.value_coef * value_loss
            return total_loss, (latents_pred, transition_loss, reward_loss, value_loss, jnp.mean(qfs_values, axis=0))

        grad_fn = jax.value_and_grad(loss_fn, argnums=(0,1,2,3), has_aux=True)
        (_, (latents_pred, transition_loss, reward_loss, value_loss, qf_values,)), grads = \
            grad_fn(encoder_state.params, dynamics_state.params, reward_state.params, qfs_state.params)

        encoder_state = encoder_state.apply_gradients(grads=grads[0])
        dynamics_state = dynamics_state.apply_gradients(grads=grads[1])
        reward_state = reward_state.apply_gradients(grads=grads[2])
        qfs_state = qfs_state.apply_gradients(grads=grads[3])
        
        return (encoder_state, dynamics_state, reward_state, qfs_state), (transition_loss, reward_loss, value_loss), qf_values, latents_pred


    @jax.jit
    def update_actor(
        key: jnp.ndarray,
        actor_state: TrainState,
        qfs_state: TrainState,
        latents_pred: jnp.ndarray,
        scale: jnp.ndarray,
    ):
        def loss_fn(params, scale):
            def calc_q_value(latent, key):
                key1, key2 = jax.random.split(key)
                _, action, logprob, _ = actor_forward(params, key1, latent)
                qfs_disc_value = jax.vmap(qf.apply, (0, None, None, None))(qfs_state.params, latent, action, False)
                qf1_disc_value, qf2_disc_value = qfs_disc_value[jax.random.choice(key2, args.critic_num, (2,), replace=False)]
                qf1_value, qf2_value = two_hot_inv(qf1_disc_value), two_hot_inv(qf2_disc_value)
                qf_value = jnp.minimum(qf1_value, qf2_value)
                return qf_value, logprob

            rhos = args.rho ** jnp.arange(args.horizon+1)
            keys = jax.random.split(key, args.horizon+1)
            qf_values, logprobs = jax.vmap(calc_q_value)(latents_pred, keys)
            scale = update_scale(qf_values[0], scale)
            qf_values = qf_values / scale
            actor_loss = jnp.mean(rhos[:, None, None] * (- qf_values + args.entropy_coef * logprobs))
            return actor_loss, scale

        (loss_value, scale), grads = jax.value_and_grad(loss_fn, has_aux=True)(actor_state.params, scale)
        actor_state = actor_state.apply_gradients(grads=grads)
        return actor_state, scale, loss_value
    

    @jax.jit
    def update_target(encoder_state, qfs_state, actor_state):
        encoder_state = encoder_state.replace(target_params=optax.incremental_update(encoder_state.params, encoder_state.target_params, args.tau))
        qfs_state = qfs_state.replace(target_params=optax.incremental_update(qfs_state.params, qfs_state.target_params, args.tau))
        actor_state = actor_state.replace(target_params=optax.incremental_update(actor_state.params, actor_state.target_params, args.tau))
        return encoder_state, qfs_state, actor_state
    

    @jax.jit
    def planning_fn(
        key: jnp.ndarray,
        obs: np.ndarray,
        encoder_state: TrainState,
        dynamics_state: TrainState,
        reward_state: TrainState,
        actor_state: TrainState,
        qfs_state: TrainState,
        actions_mean: jnp.ndarray,
    ):

        latent_init = encoder.apply(encoder_state.params, obs)

        def get_actor_action_sequence(key):
            def scan_fn(latent, key):
                action = actor_forward(actor_state.params, key, latent)[1]
                latent_pred = dynamics.apply(dynamics_state.params, latent, action)
                return latent_pred, action

            keys = jax.random.split(key, args.horizon)
            latent_init_batch = jnp.repeat(latent_init[None], args.num_actor_traj, axis=0)
            _, actions_batch = jax.lax.scan(scan_fn, latent_init_batch, keys)
            actions_batch = jnp.transpose(actions_batch, (1, 0, 2))
            return actions_batch

        key, subkey = jax.random.split(key)
        action_actor_batch = get_actor_action_sequence(subkey)

        def sample_action_sequence(key, actions_mean, actions_std):
            eps = jax.random.normal(key, (args.num_samples, args.horizon, action_dim))
            actions_batch = actions_mean[None] + actions_std[None] * eps
            actions_batch = jnp.clip(actions_batch, -1.0, 1.0)
            return actions_batch

        def compute_return(key, latent, actions):
            def rollout(carry, action):
                latent, discount = carry
                next_latent = dynamics.apply(dynamics_state.params, latent, action)
                reward_disc = reward_fn.apply(reward_state.params, latent, action)
                reward = two_hot_inv(reward_disc)
                return (next_latent, args.gamma*discount), discount*reward

            key1, key2 = jax.random.split(key)
            (latent_last, discount), rewards = jax.lax.scan(rollout, (latent, 1.0), actions)
            action = actor_forward(actor_state.params, key1, latent_last)[1]
            qfs_disc_value = jax.vmap(qf.apply, (0, None, None, None))(qfs_state.params, latent_last, action, False)
            qf1_disc_value, qf2_disc_value = qfs_disc_value[jax.random.choice(key2, args.critic_num, (2,), replace=False)]
            qf1_value, qf2_value = two_hot_inv(qf1_disc_value), two_hot_inv(qf2_disc_value)
            qf_value = jnp.minimum(qf1_value, qf2_value)
            V = jnp.sum(rewards) + discount * qf_value
            return V

        def get_elite(key, actions_mean, actions_std):
            key, subkey = jax.random.split(key)
            actions_batch = jnp.concatenate([
                sample_action_sequence(subkey, actions_mean, actions_std),
                action_actor_batch,
            ])

            keys = jax.random.split(key, len(actions_batch))
            V_func = lambda key, actions: compute_return(key, latent_init, actions)
            V_batch = jax.vmap(V_func)(keys, actions_batch)
            V_elite, elite_index = jax.lax.top_k(V_batch[:, 0], args.num_elites)
            actions_elite = actions_batch[elite_index]
            return V_elite, actions_elite

        def update_params(mean, std, V_elite, actions_elite):
            score = jnp.exp(args.temperature * (V_elite - jnp.max(V_elite)))
            score = score[:, None, None] / (jnp.sum(score) + 1e-9)
            mean = jnp.sum(score * actions_elite, axis=0)
            std = jnp.sqrt(jnp.sum(score * (actions_elite - mean[None])**2, axis=0))
            std = jnp.clip(std, args.min_std, args.max_std)
            return mean, std

        def body_fun(_, val):
            key, actions_mean, actions_std = val
            key, subkey = jax.random.split(key)
            V_elite, actions_elite = get_elite(subkey, actions_mean, actions_std)
            actions_mean, actions_std \
                = update_params(actions_mean, actions_std, V_elite, actions_elite)
            return (key, actions_mean, actions_std)

        actions_std = jnp.ones((args.horizon, action_dim))
        init_val = (key, actions_mean, actions_std)
        val = jax.lax.fori_loop(0, args.iterations, body_fun, init_val)
        key, actions_mean, actions_std = val

        key, subkey = jax.random.split(key)
        V_elite, actions_elite = get_elite(subkey, actions_mean, actions_std)
        action_best = actions_elite[jnp.argmax(V_elite), 0]

        return action_best, actions_mean
    
    rb = ReplayBuffer(
        args.total_timesteps, 
        obs_dim, 
        action_dim,
        args.horizon
    )

    start_time = time.time()
    actions_mean = jnp.zeros((args.horizon, action_dim))
    termination, truncation = True, True
    total_update = 0
    scale = jnp.array([1.0])

    for global_step in range(0, args.total_timesteps, args.action_repeat):

        if not (termination or truncation):

            if global_step < args.learning_starts:
                action = env.action_space.sample()

            else:
                key, subkey = jax.random.split(key)
                action, actions_mean = planning_fn(
                    subkey,
                    obs,
                    encoder_state,
                    dynamics_state,
                    reward_state,
                    actor_state,
                    qfs_state,
                    actions_mean,
                )
                actions_mean = jnp.concatenate([actions_mean[1:], jnp.zeros((1, action_dim))], axis=0)
                key, subkey = jax.random.split(key)
                eps = jax.random.normal(subkey, (action_dim,))
                action = action + args.exploration_noise * eps
                action = np.array(action)

            obs, reward, termination, truncation, info = env.step(action)
            rb.add(obs, action, reward, termination, truncation)

        else:
            if 'episode' in info:
                print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
                
            obs, _ = env.reset()
            actions_mean = jnp.zeros((args.horizon, action_dim))
            termination, truncation = False, False
            rb.add(obs, np.zeros(action_dim), np.zeros(1), termination, truncation)


        if global_step > args.learning_starts:

            obss, actions, rewards, terminations = rb.sample(args.batch_size)
            key, subkey = jax.random.split(key)
            (encoder_state, dynamics_state, reward_state, qfs_state), (transition_loss, reward_loss, value_loss), qf_values, latents_pred = \
                update_dynamics_and_critic(
                    subkey, 
                    encoder_state, 
                    dynamics_state, 
                    reward_state, 
                    actor_state, 
                    qfs_state, 
                    obss, 
                    actions, 
                    rewards,
                    terminations,
                )

            key, subkey = jax.random.split(key)
            actor_state, scale, actor_loss = \
                update_actor(subkey, actor_state, qfs_state, latents_pred, scale)
        
            if total_update % args.target_network_frequency == 0:
                encoder_state, qfs_state, actor_state = \
                    update_target(encoder_state, qfs_state, actor_state)

            if total_update % 100 == 0:
                writer.add_scalar("losses/transition_loss", transition_loss.item(), global_step)
                writer.add_scalar("losses/reward_loss", reward_loss.item(), global_step)
                writer.add_scalar("charts/qfs_values", np.mean(qf_values).item(), global_step)
                writer.add_scalar("losses/qfs_loss", value_loss.item(), global_step)
                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
                writer.add_scalar("charts/scale", np.array(scale).item(), global_step)
                print("SPS:", int(global_step / (time.time() - start_time)))
                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)

            total_update += 1

    env.close()
    writer.close()
